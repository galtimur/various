{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": "from analyze_activations import analyze_activations",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T09:14:58.395024Z",
     "start_time": "2025-09-18T09:14:53.791300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "activations = analyze_activations(model = \"Qwen/Qwen3-8B\",\n",
    "  dataset = \"ag_news\",\n",
    "  split = \"train\",\n",
    "  text_column = None,\n",
    "  num_samples = 128,\n",
    "  max_length = 512,\n",
    "  batch_size = 1,\n",
    "  bins = 201,\n",
    "  min_val = -10.0,\n",
    "  max_val = 10.0,\n",
    "  output_dir = None,\n",
    "  prefer_bf16 = True)"
   ],
   "id": "19cb283c81ab92a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: Qwen/Qwen3-8B\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8f33488129bb4f5c99ad026be9141759"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Timur.Galimzyanov\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\kotlin-initiative-agynq_Rm-py3.12\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Timur.Galimzyanov\\.cache\\huggingface\\hub\\models--Qwen--Qwen3-8B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "abe07f645f3048078bae5a2e6c278b58"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d3666973405a48b692843d65f1956005"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4a482f17deed41a096687565c1486b67"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: Qwen/Qwen3-8B\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/728 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cc78f8dc13534c729061f70219222f42"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "ename": "ValueError",
     "evalue": "The checkpoint you are trying to load has model type `qwen3` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\nYou can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git`",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\kotlin-initiative-agynq_Rm-py3.12\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:1092\u001B[0m, in \u001B[0;36mAutoConfig.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[0;32m   1091\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1092\u001B[0m     config_class \u001B[38;5;241m=\u001B[39m \u001B[43mCONFIG_MAPPING\u001B[49m\u001B[43m[\u001B[49m\u001B[43mconfig_dict\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodel_type\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m   1093\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\kotlin-initiative-agynq_Rm-py3.12\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:794\u001B[0m, in \u001B[0;36m_LazyConfigMapping.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m    793\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mapping:\n\u001B[1;32m--> 794\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key)\n\u001B[0;32m    795\u001B[0m value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mapping[key]\n",
      "\u001B[1;31mKeyError\u001B[0m: 'qwen3'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43manalyze_activations\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mQwen/Qwen3-8B\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mag_news\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msplit\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext_column\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_samples\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m128\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m512\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbins\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m201\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmin_val\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m10.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_val\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m10.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefer_bf16\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Timur\\Varios for job\\projects\\various\\llm_analysis\\analyze_activations.py:184\u001B[0m, in \u001B[0;36manalyze_activations\u001B[1;34m(model, dataset, split, text_column, num_samples, max_length, batch_size, bins, min_val, max_val, output_dir, prefer_bf16)\u001B[0m\n\u001B[0;32m    181\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(model, use_fast\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    183\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoading model: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 184\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    185\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    186\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    187\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mdevice_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    188\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    189\u001B[0m model \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[0;32m    191\u001B[0m ensure_pad_token(tokenizer, model)\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\kotlin-initiative-agynq_Rm-py3.12\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:526\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m    523\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquantization_config\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    524\u001B[0m     _ \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquantization_config\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 526\u001B[0m config, kwargs \u001B[38;5;241m=\u001B[39m \u001B[43mAutoConfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    527\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    528\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_unused_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    529\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    530\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcode_revision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcode_revision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    531\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_commit_hash\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcommit_hash\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    532\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    533\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    534\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    536\u001B[0m \u001B[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001B[39;00m\n\u001B[0;32m    537\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs_orig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch_dtype\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\kotlin-initiative-agynq_Rm-py3.12\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:1094\u001B[0m, in \u001B[0;36mAutoConfig.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[0;32m   1092\u001B[0m         config_class \u001B[38;5;241m=\u001B[39m CONFIG_MAPPING[config_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_type\u001B[39m\u001B[38;5;124m\"\u001B[39m]]\n\u001B[0;32m   1093\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m:\n\u001B[1;32m-> 1094\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1095\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe checkpoint you are trying to load has model type `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig_dict[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel_type\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1096\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut Transformers does not recognize this architecture. This could be because of an \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1097\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124missue with the checkpoint, or because your version of Transformers is out of date.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1098\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou can update Transformers with the command `pip install --upgrade transformers`. If this \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1099\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdoes not work, and the checkpoint is very new, then there may not be a release version \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1100\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthat supports this model yet. In this case, you can get the most up-to-date code by installing \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1101\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTransformers from source with the command \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1102\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`pip install git+https://github.com/huggingface/transformers.git`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1103\u001B[0m         )\n\u001B[0;32m   1104\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m config_class\u001B[38;5;241m.\u001B[39mfrom_dict(config_dict, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39munused_kwargs)\n\u001B[0;32m   1105\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1106\u001B[0m     \u001B[38;5;66;03m# Fallback: use pattern matching on the string.\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m     \u001B[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: The checkpoint you are trying to load has model type `qwen3` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\nYou can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git`"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ce938790bffb4fb7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
